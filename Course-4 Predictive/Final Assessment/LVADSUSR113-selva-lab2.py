# -*- coding: utf-8 -*-
"""Lab2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t60vwekPQVHgqHFw4wPpED_K4mT-gSvd
"""

import pandas as pd

df_auto = pd.read_csv('/content/auto-mpg.csv')
df_auto.info()

df_auto.describe()

df_auto.head(1)

from matplotlib import pyplot as plt
import seaborn as sns
def _plot_series(series, series_name, series_index=0):
  palette = list(sns.palettes.mpl_palette('Dark2'))
  xs = series['model year']
  ys = series['mpg']

  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = df_auto.sort_values('model year', ascending=True)
_plot_series(df_sorted, '')
sns.despine(fig=fig, ax=ax)
plt.xlabel('model year')
_ = plt.ylabel('mpg')

from matplotlib import pyplot as plt
import seaborn as sns
def _plot_series(series, series_name, series_index=0):
  palette = list(sns.palettes.mpl_palette('Dark2'))
  xs = series['weight']
  ys = series['mpg']

  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = df_auto.sort_values('weight', ascending=True)
_plot_series(df_sorted, '')
sns.despine(fig=fig, ax=ax)
plt.xlabel('weight')
_ = plt.ylabel('mpg')

df_auto.isnull().sum()

df_auto.bfill(inplace=True)

df_auto.isnull().sum()

duplicates = df_auto.duplicated(keep=False)
df_auto['dup_bool'] = duplicates
print(df_auto[df_auto['dup_bool'] == True].count())
df_auto.drop('dup_bool',axis=1)
df_auto.head(1)

from sklearn.preprocessing import LabelEncoder

lbl_enc = LabelEncoder()

from sklearn.model_selection import train_test_split


x = df_auto.drop(['car name','origin','mpg','horsepower'],axis=1)
y = df_auto['mpg']

X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.33, random_state=42)

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
import xgboost as xgb
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import numpy as np

dec_reg = DecisionTreeRegressor()
dec_reg.fit(X_train,y_train)
y_pred = dec_reg.predict(X_test)
MSE = mean_squared_error(y_test, y_pred)
RMSE = np.sqrt(MSE)
r2 = r2_score(y_test, y_pred)
print("Decision tree \nMSE:", MSE)
print("RMSE:", RMSE)
print("R2 score:", r2)



lr_reg = LinearRegression()
lr_reg.fit(X_train,y_train)
y_pred = lr_reg.predict(X_test)
MSE = mean_squared_error(y_test, y_pred)
RMSE = np.sqrt(MSE)
r2 = r2_score(y_test, y_pred)
print("Linear regression \nMSE:", MSE)
print("RMSE:", RMSE)
print("R2 score:", r2)



RF_reg = RandomForestRegressor()
RF_reg.fit(X_train,y_train)
y_pred = RF_reg.predict(X_test)
MSE = mean_squared_error(y_test, y_pred)
RMSE = np.sqrt(MSE)
r2 = r2_score(y_test, y_pred)
print("Random Forest \nMSE:", MSE)
print("RMSE:", RMSE)
print("R2 score:", r2)



xgb_reg = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
xgb_reg.fit(X_train, y_train)
y_pred = xgb_reg.predict(X_test)
MSE = mean_squared_error(y_test, y_pred)
RMSE = np.sqrt(MSE)
r2 = r2_score(y_test, y_pred)
print("XGB \nMSE:", MSE)
print("RMSE:", RMSE)
print("R2 score:", r2)

# Random forest seems to have least errors